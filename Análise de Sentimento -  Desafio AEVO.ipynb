{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Desafio Aevo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ª Por quais processos você passaria até a entrega do seu projeto?\n",
    "\n",
    "## 2ª Para uma demanda de extração de palavras chaves (tags) de um texto, quais ferramentas e técnicas você julga primordiais no processamento de dados para obter bons resultados?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP (Natural Language Processing) ou PLN (Processamemento de Linguagem Natural)\n",
    "  * É uma área dentro da Inteligência Artificial (IA) que se dedica a desenvolver a capacidade da tecnologia em entender a linguagem dos seres humanos.\n",
    "  * Serve como um tradutor que permite que a tecnologia entenda o usuário mesmo ele usando a linguagem natural.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Regra de Negócio\n",
    "  * Entender, estudar, procurar sobre o problema proposto\n",
    "  \n",
    "##### Coleta de Dados\n",
    "  * Existem várias maneiras de coletar dados. Podem ser através de uma planilha Excel, arquivo csv ou json, redes sociais, Bancos relacionais e não relacionais. \n",
    "  \n",
    "##### Analisar e Explorar\n",
    "  * Busca por insights, padrões, valores outliers. Entender melhores como os dados se comportam, gerando gráficos, análise estatística, entre outras ferramentas.\n",
    "\n",
    "##### Pré-Processamento\n",
    "  * Tem como objetivo fazer uma limpeza e/ou melhorar a qualidade dos dados\n",
    "    * tokenização: Objetivo de separar palavras ou sentenças em unidades\n",
    "      * n-gram: É uma sequência de \"n\" elementos em uma sequência maior. Existem alguns tipos de n-gram\n",
    "        * unigram: Separação da sentença em um quantidade de termo igual a 1\n",
    "        * bigrama: Quantidade de termo igual a 2\n",
    "        * trigrama: Quantidade de termo igual a 3\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![image.png](https://www.oreilly.com/library/view/artificial-intelligence-for/9781788472173/assets/447da4ba-e40a-4787-a2f2-20a906077475.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   * Stopwords: Palavras irrelevantes no contexto em que trabalha. Geralmente são preposições, artigos, alguns advérbios e alguns verbos. O próprio cientista de dado pode escolher as stopwords.\n",
    "   * Part-of-speech tagger: Assinala, para cada palavra, a classe gramatical a qual pertence.\n",
    "   * Stemmer: é a técnica de remover sufixos e prefixos de uma palavra. Ex. Frequentou, Frequentava > Frequent\n",
    "   * Lemmatizer: Parecido com stemming, porém, substitui a palavra por um existente. Ex. Frequentou, Frequentava > Frequenta\n",
    "   * Medida: Bag of Words, TF, TF-IDF\n",
    "   \n",
    "##### Modelagem\n",
    "  * Escolha do algoritmo de machine learning para resolução do problema.\n",
    "\n",
    "##### Avaliação do Modelo\n",
    "  * Métricas para comparar quem teve o melhor rendimento e também o Trade-Off gerado, algoritmo x rendimento.\n",
    "  * Acúracia, Precisão, Recall. F-score são métricas para problemas de classificação.\n",
    "  * GridSearch: Otimização de Hiperparâmetros\n",
    "  \n",
    "##### Apresentação do Resultado\n",
    "  * Entrega do Produto\n",
    "  \n",
    "##### Observação\n",
    "  * O processamento e análise dos dados podem ser feitas de duas maneiras:    \n",
    "    * Batch: Processamento de um arquivo ou dataset finito. Utilizado geralmente em grande volume de dados.\n",
    "    * Streaming: Processa dados em um fluxo contínuo. Utizado para monitoramento de serviço."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplo de coleta de dados\n",
    "\n",
    "No próprio banco relacional, MySQL, tem alguns dados para utilização e prática."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "import pymysql\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando conexão\n",
    "engine = sqlalchemy.create_engine('mysql+pymysql://root:120593@localhost:3306/world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>CountryCode</th>\n",
       "      <th>District</th>\n",
       "      <th>Population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Kabul</td>\n",
       "      <td>AFG</td>\n",
       "      <td>Kabol</td>\n",
       "      <td>1780000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Qandahar</td>\n",
       "      <td>AFG</td>\n",
       "      <td>Qandahar</td>\n",
       "      <td>237500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Herat</td>\n",
       "      <td>AFG</td>\n",
       "      <td>Herat</td>\n",
       "      <td>186800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Mazar-e-Sharif</td>\n",
       "      <td>AFG</td>\n",
       "      <td>Balkh</td>\n",
       "      <td>127800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>NLD</td>\n",
       "      <td>Noord-Holland</td>\n",
       "      <td>731200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID            Name CountryCode       District  Population\n",
       "0   1           Kabul         AFG          Kabol     1780000\n",
       "1   2        Qandahar         AFG       Qandahar      237500\n",
       "2   3           Herat         AFG          Herat      186800\n",
       "3   4  Mazar-e-Sharif         AFG          Balkh      127800\n",
       "4   5       Amsterdam         NLD  Noord-Holland      731200"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trazendo dados do banco relacional\n",
    "df = pd.read_sql_table('city', engine)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kabul</td>\n",
       "      <td>1780000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Qandahar</td>\n",
       "      <td>237500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Herat</td>\n",
       "      <td>186800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mazar-e-Sharif</td>\n",
       "      <td>127800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>731200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Name  Population\n",
       "0           Kabul     1780000\n",
       "1        Qandahar      237500\n",
       "2           Herat      186800\n",
       "3  Mazar-e-Sharif      127800\n",
       "4       Amsterdam      731200"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Outra maneira de trazer os dados do banco relacional\n",
    "df_2 = pd.read_sql_query('select Name,Population from city', engine)\n",
    "df_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise de Sentimento\n",
    "\n",
    "Analisar um dataset de tweets que possui como palavra chave \"BH\". Explorar os dados, limpar, criar um modelo de machine learning, avaliar o modelo, e por fim, salvar o modelo que apresentar melhor retorno."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conhecendo o DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando os Dados\n",
    "dataframe = pd.read_csv('Tweets_Mg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Created At</th>\n",
       "      <th>Text</th>\n",
       "      <th>Geo Coordinates.latitude</th>\n",
       "      <th>Geo Coordinates.longitude</th>\n",
       "      <th>User Location</th>\n",
       "      <th>Username</th>\n",
       "      <th>User Screen Name</th>\n",
       "      <th>Retweet Count</th>\n",
       "      <th>Classificacao</th>\n",
       "      <th>...</th>\n",
       "      <th>Unnamed: 15</th>\n",
       "      <th>Unnamed: 16</th>\n",
       "      <th>Unnamed: 17</th>\n",
       "      <th>Unnamed: 18</th>\n",
       "      <th>Unnamed: 19</th>\n",
       "      <th>Unnamed: 20</th>\n",
       "      <th>Unnamed: 21</th>\n",
       "      <th>Unnamed: 22</th>\n",
       "      <th>Unnamed: 23</th>\n",
       "      <th>Unnamed: 24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Sun Jan 08 01:22:05 +0000 2017</td>\n",
       "      <td>���⛪ @ Catedral de Santo Antônio - Governador ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Brasil</td>\n",
       "      <td>Leonardo C Schneider</td>\n",
       "      <td>LeoCSchneider</td>\n",
       "      <td>0</td>\n",
       "      <td>Neutro</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Sun Jan 08 01:49:01 +0000 2017</td>\n",
       "      <td>� @ Governador Valadares, Minas Gerais https:/...</td>\n",
       "      <td>-41.9333</td>\n",
       "      <td>-18.85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wândell</td>\n",
       "      <td>klefnews</td>\n",
       "      <td>0</td>\n",
       "      <td>Neutro</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Sun Jan 08 01:01:46 +0000 2017</td>\n",
       "      <td>�� @ Governador Valadares, Minas Gerais https:...</td>\n",
       "      <td>-41.9333</td>\n",
       "      <td>-18.85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wândell</td>\n",
       "      <td>klefnews</td>\n",
       "      <td>0</td>\n",
       "      <td>Neutro</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Wed Jan 04 21:43:51 +0000 2017</td>\n",
       "      <td>��� https://t.co/BnDsO34qK0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ana estudando</td>\n",
       "      <td>estudandoconcur</td>\n",
       "      <td>0</td>\n",
       "      <td>Neutro</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Mon Jan 09 15:08:21 +0000 2017</td>\n",
       "      <td>��� PSOL vai questionar aumento de vereadores ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Emily</td>\n",
       "      <td>Milly777</td>\n",
       "      <td>0</td>\n",
       "      <td>Negativo</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                      Created At  \\\n",
       "0           0  Sun Jan 08 01:22:05 +0000 2017   \n",
       "1           1  Sun Jan 08 01:49:01 +0000 2017   \n",
       "2           2  Sun Jan 08 01:01:46 +0000 2017   \n",
       "3           3  Wed Jan 04 21:43:51 +0000 2017   \n",
       "4           4  Mon Jan 09 15:08:21 +0000 2017   \n",
       "\n",
       "                                                Text  \\\n",
       "0  ���⛪ @ Catedral de Santo Antônio - Governador ...   \n",
       "1  � @ Governador Valadares, Minas Gerais https:/...   \n",
       "2  �� @ Governador Valadares, Minas Gerais https:...   \n",
       "3                        ��� https://t.co/BnDsO34qK0   \n",
       "4  ��� PSOL vai questionar aumento de vereadores ...   \n",
       "\n",
       "   Geo Coordinates.latitude  Geo Coordinates.longitude User Location  \\\n",
       "0                       NaN                        NaN        Brasil   \n",
       "1                  -41.9333                     -18.85           NaN   \n",
       "2                  -41.9333                     -18.85           NaN   \n",
       "3                       NaN                        NaN           NaN   \n",
       "4                       NaN                        NaN           NaN   \n",
       "\n",
       "               Username User Screen Name  Retweet Count Classificacao  ...  \\\n",
       "0  Leonardo C Schneider    LeoCSchneider              0        Neutro  ...   \n",
       "1               Wândell         klefnews              0        Neutro  ...   \n",
       "2               Wândell         klefnews              0        Neutro  ...   \n",
       "3         Ana estudando  estudandoconcur              0        Neutro  ...   \n",
       "4                 Emily         Milly777              0      Negativo  ...   \n",
       "\n",
       "  Unnamed: 15  Unnamed: 16  Unnamed: 17  Unnamed: 18  Unnamed: 19  \\\n",
       "0         NaN          NaN          NaN          NaN          NaN   \n",
       "1         NaN          NaN          NaN          NaN          NaN   \n",
       "2         NaN          NaN          NaN          NaN          NaN   \n",
       "3         NaN          NaN          NaN          NaN          NaN   \n",
       "4         NaN          NaN          NaN          NaN          NaN   \n",
       "\n",
       "   Unnamed: 20  Unnamed: 21  Unnamed: 22  Unnamed: 23  Unnamed: 24  \n",
       "0          NaN          NaN          NaN          NaN          NaN  \n",
       "1          NaN          NaN          NaN          NaN          NaN  \n",
       "2          NaN          NaN          NaN          NaN          NaN  \n",
       "3          NaN          NaN          NaN          NaN          NaN  \n",
       "4          NaN          NaN          NaN          NaN          NaN  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conhecendo o Dataset\n",
    "dataframe.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8199 entries, 0 to 8198\n",
      "Data columns (total 26 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   Unnamed: 0                 8199 non-null   int64  \n",
      " 1   Created At                 8199 non-null   object \n",
      " 2   Text                       8199 non-null   object \n",
      " 3   Geo Coordinates.latitude   104 non-null    float64\n",
      " 4   Geo Coordinates.longitude  104 non-null    float64\n",
      " 5   User Location              5489 non-null   object \n",
      " 6   Username                   8199 non-null   object \n",
      " 7   User Screen Name           8199 non-null   object \n",
      " 8   Retweet Count              8199 non-null   int64  \n",
      " 9   Classificacao              8199 non-null   object \n",
      " 10  Observação                 1 non-null      object \n",
      " 11  Unnamed: 10                0 non-null      float64\n",
      " 12  Unnamed: 11                0 non-null      float64\n",
      " 13  Unnamed: 12                0 non-null      float64\n",
      " 14  Unnamed: 13                0 non-null      float64\n",
      " 15  Unnamed: 14                0 non-null      float64\n",
      " 16  Unnamed: 15                0 non-null      float64\n",
      " 17  Unnamed: 16                0 non-null      float64\n",
      " 18  Unnamed: 17                0 non-null      float64\n",
      " 19  Unnamed: 18                0 non-null      float64\n",
      " 20  Unnamed: 19                0 non-null      float64\n",
      " 21  Unnamed: 20                0 non-null      float64\n",
      " 22  Unnamed: 21                0 non-null      float64\n",
      " 23  Unnamed: 22                0 non-null      float64\n",
      " 24  Unnamed: 23                0 non-null      float64\n",
      " 25  Unnamed: 24                0 non-null      float64\n",
      "dtypes: float64(17), int64(2), object(7)\n",
      "memory usage: 1.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# Informações\n",
    "dataframe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5765, 26)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removendo as linhas duplicadas\n",
    "dataframe_novo = dataframe.drop_duplicates(['Text'])\n",
    "dataframe_novo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separando o dataframe em variável independente e dependente \n",
    "x = dataframe_novo[['Text']]\n",
    "y = dataframe_novo['Classificacao']\n",
    "\n",
    "# Transformando y em números\n",
    "le = LabelEncoder()\n",
    "y_final = le.fit_transform(y)\n",
    "\n",
    "# Positivo -> 2 , Neutro -> 1 , Negativo -> 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1effa1235c8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Variável de saída\n",
    "sns.barplot(y.value_counts().index, y.value_counts().values, palette=\"Blues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré Processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "#### Primeira forma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para remover stopwords\n",
    "def remove_stopwords(instancia):\n",
    "    \n",
    "    # Lista de Stopwords\n",
    "    stopwords = nltk.corpus.stopwords.words('Portuguese') + [',','.']\n",
    "    \n",
    "    # Split das frases\n",
    "    palavras = [i for i in instancia.split() if not i in stopwords]\n",
    "    return (\" \".join(palavras))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Segunda forma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tokenizer_twitter = TweetTokenizer()\n",
    "\n",
    "# Outra função para remover stopwords\n",
    "def remove_stopwords_2(instancia):\n",
    "    \n",
    "    # Lista de Stopwords\n",
    "    stopwords = nltk.corpus.stopwords.words('Portuguese') + [',','.']\n",
    "    \n",
    "    # Tokenização\n",
    "    palavras = [i for i in tokenizer_twitter.tokenize(instancia) if not i in stopwords]\n",
    "    return (\" \".join(palavras))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "frase = 'Alvim. está fazendo um desafio para entrar na equipe da Aevo,'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alvim. fazendo desafio entrar equipe Aevo,'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Utilizando a primeira função\n",
    "remove_stopwords(frase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alvim fazendo desafio entrar equipe Aevo'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Utilizando a segunda função\n",
    "remove_stopwords_2(frase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpeza, Transformação e Criação de variável"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando função 1:\n",
    "x1 = x.copy()\n",
    "x1['Text'] = x1['Text'].str.lower()\n",
    "x1['Text'] = [remove_stopwords(i) for i in x1['Text']]\n",
    "\n",
    "# Aplicando função 2:\n",
    "x_2 = x.copy()\n",
    "x_2['Text'] = x_2['Text'].str.lower()\n",
    "x_2['Text'] = [remove_stopwords_2(i) for i in x_2['Text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para contar quantidade de palavras por linha\n",
    "def qntd_palavra(instancia):\n",
    "    \n",
    "    qnt = len(instancia.split())\n",
    "    \n",
    "    return qnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando variável \"Quantidade\"\n",
    "x1['Quantidade'] = [qntd_palavra(i) for i in x1['Text']]\n",
    "x_2['Quantidade'] = [qntd_palavra(i) for i in x_2['Text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Quantidade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>���⛪ @ catedral santo antônio - governador val...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>� @ governador valadares, minas gerais https:/...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>�� @ governador valadares, minas gerais https:...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Quantidade\n",
       "0  ���⛪ @ catedral santo antônio - governador val...           9\n",
       "1  � @ governador valadares, minas gerais https:/...           7\n",
       "2  �� @ governador valadares, minas gerais https:...           7"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Como ficou o x1\n",
    "x1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Quantidade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>� � � ⛪ @ catedral santo antônio - governador ...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>� @ governador valadares minas gerais https://...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>� � @ governador valadares minas gerais https:...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Quantidade\n",
       "0  � � � ⛪ @ catedral santo antônio - governador ...          14\n",
       "1  � @ governador valadares minas gerais https://...           7\n",
       "2  � � @ governador valadares minas gerais https:...           8"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Como ficou o x_2\n",
    "x_2.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Medida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alvim Serrao\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5765, 12970) (5765, 13234)\n"
     ]
    }
   ],
   "source": [
    "# Bag of words\n",
    "count_vect = CountVectorizer()\n",
    "count_vect_2 = CountVectorizer(tokenizer=tokenizer_twitter.tokenize)\n",
    "\n",
    "# Treinamento\n",
    "count_vect.fit(x1['Text'])\n",
    "count_vect_2.fit(x_2['Text'])\n",
    "\n",
    "# Transformação\n",
    "x_transf = count_vect.transform(x1['Text'])\n",
    "x_transf_2 = count_vect_2.transform(x_2['Text'])\n",
    "\n",
    "# Tamanho dos conjunto de dados\n",
    "print(x_transf.shape, x_transf_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_transf_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2635 10397 1645\n"
     ]
    }
   ],
   "source": [
    "print(count_vect.vocabulary_['catedral'],count_vect.vocabulary_['santo'],count_vect.vocabulary_['antônio'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5765, 12970) (5765, 13234)\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF:\n",
    "tf_transform = TfidfTransformer(norm='l1')\n",
    "tf_transform_2 = TfidfTransformer(norm='l1')\n",
    "\n",
    "# Ajuste e Transformação dos dados\n",
    "x_Tdidf_1 = tf_transform.fit_transform(x_transf)\n",
    "x_Tdidf_2 = tf_transform_2.fit_transform(x_transf_2)\n",
    "\n",
    "# Tamanho dos conjunto de dados\n",
    "print(x_Tdidf_1.shape, x_Tdidf_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo de Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instâncias de teste dentro de uma lista\n",
    "testes = ['Esse governo está no início, vamos ver o que vai dar',\n",
    "          'Estou muito feliz com o governo de Minas esse ano',\n",
    "          'O estado de Minas Gerais decretou calamidade financeira!!!',\n",
    "          'A segurança desse país está deixando a desejar',\n",
    "          'O governador de Minas é mais uma vez do PT',\n",
    "          'Homem é preso em bh',\n",
    "          ',,,,,',\n",
    "          'Cartão BHBus dará vantagens para usuários do transporte coletivo de BH',\n",
    "          'funk de bh = melhor do brasil',\n",
    "          'queria ir em bh de novo :c',\n",
    "          'Partiu BH ✈'\n",
    "         ]\n",
    "\n",
    "resposta_teste = [1,2,0,0,1,2,1,2,2,1,1]\n",
    "\n",
    "# Transformando os dados de testes\n",
    "testes_vect = count_vect.transform(testes)\n",
    "testes_vect_2 = count_vect_2.transform(testes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A frase 'Esse governo está no início, vamos ver o que vai dar' é: Neutro\n",
      "A frase 'Estou muito feliz com o governo de Minas esse ano' é: Neutro\n",
      "A frase 'O estado de Minas Gerais decretou calamidade financeira!!!' é: Negativo\n",
      "A frase 'A segurança desse país está deixando a desejar' é: Neutro\n",
      "A frase 'O governador de Minas é mais uma vez do PT' é: Negativo\n",
      "A frase 'Homem é preso em bh' é: Positivo\n",
      "A frase ',,,,,' é: Positivo\n",
      "A frase 'Cartão BHBus dará vantagens para usuários do transporte coletivo de BH' é: Neutro\n",
      "A frase 'funk de bh = melhor do brasil' é: Neutro\n",
      "A frase 'queria ir em bh de novo :c' é: Neutro\n",
      "A frase 'Partiu BH ✈' é: Neutro\n",
      "-----------------------------------------------------------------\n",
      "A frase 'Esse governo está no início, vamos ver o que vai dar' é: Neutro\n",
      "A frase 'Estou muito feliz com o governo de Minas esse ano' é: Neutro\n",
      "A frase 'O estado de Minas Gerais decretou calamidade financeira!!!' é: Negativo\n",
      "A frase 'A segurança desse país está deixando a desejar' é: Neutro\n",
      "A frase 'O governador de Minas é mais uma vez do PT' é: Neutro\n",
      "A frase 'Homem é preso em bh' é: Positivo\n",
      "A frase ',,,,,' é: Positivo\n",
      "A frase 'Cartão BHBus dará vantagens para usuários do transporte coletivo de BH' é: Positivo\n",
      "A frase 'funk de bh = melhor do brasil' é: Neutro\n",
      "A frase 'queria ir em bh de novo :c' é: Neutro\n",
      "A frase 'Partiu BH ✈' é: Neutro\n"
     ]
    }
   ],
   "source": [
    "# Primeiro Modelo\n",
    "modelo_nb = MultinomialNB()\n",
    "modelo_nb.fit(x_transf, y_final)\n",
    "\n",
    "# Segundo Modelo\n",
    "modelo_nb_2 = MultinomialNB()\n",
    "modelo_nb_2.fit(x_transf_2, y_final)\n",
    "\n",
    "# Previsão\n",
    "resposta_1 = le.inverse_transform(modelo_nb.predict(testes_vect))\n",
    "resposta_2 = le.inverse_transform(modelo_nb_2.predict(testes_vect_2))\n",
    "\n",
    "for a, b in zip(testes, resposta_1):\n",
    "    print(f\"A frase '{a}' é: {b}\")\n",
    "print('-----------------------------------------------------------------')\n",
    "for a, b in zip(testes, resposta_2):\n",
    "    print(f\"A frase '{a}' é: {b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM - Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A frase 'Esse governo está no início, vamos ver o que vai dar' é: Negativo\n",
      "A frase 'Estou muito feliz com o governo de Minas esse ano' é: Neutro\n",
      "A frase 'O estado de Minas Gerais decretou calamidade financeira!!!' é: Neutro\n",
      "A frase 'A segurança desse país está deixando a desejar' é: Neutro\n",
      "A frase 'O governador de Minas é mais uma vez do PT' é: Neutro\n",
      "A frase 'Homem é preso em bh' é: Neutro\n",
      "A frase ',,,,,' é: Neutro\n",
      "A frase 'Cartão BHBus dará vantagens para usuários do transporte coletivo de BH' é: Neutro\n",
      "A frase 'funk de bh = melhor do brasil' é: Neutro\n",
      "A frase 'queria ir em bh de novo :c' é: Neutro\n",
      "A frase 'Partiu BH ✈' é: Neutro\n",
      "-----------------------------------------------------------------\n",
      "A frase 'Esse governo está no início, vamos ver o que vai dar' é: Negativo\n",
      "A frase 'Estou muito feliz com o governo de Minas esse ano' é: Neutro\n",
      "A frase 'O estado de Minas Gerais decretou calamidade financeira!!!' é: Negativo\n",
      "A frase 'A segurança desse país está deixando a desejar' é: Neutro\n",
      "A frase 'O governador de Minas é mais uma vez do PT' é: Negativo\n",
      "A frase 'Homem é preso em bh' é: Positivo\n",
      "A frase ',,,,,' é: Neutro\n",
      "A frase 'Cartão BHBus dará vantagens para usuários do transporte coletivo de BH' é: Neutro\n",
      "A frase 'funk de bh = melhor do brasil' é: Neutro\n",
      "A frase 'queria ir em bh de novo :c' é: Neutro\n",
      "A frase 'Partiu BH ✈' é: Neutro\n"
     ]
    }
   ],
   "source": [
    "# Primeiro Modelo\n",
    "modelo_svc = LinearSVC()\n",
    "modelo_svc.fit(x_transf, y_final)\n",
    "\n",
    "# Segundo Modelo\n",
    "modelo_svc_2 = LinearSVC()\n",
    "modelo_svc_2.fit(x_transf_2, y_final)\n",
    "\n",
    "# Previsão\n",
    "resposta_3 = le.inverse_transform(modelo_svc.predict(testes_vect))\n",
    "resposta_4 = le.inverse_transform(modelo_svc_2.predict(testes_vect_2))\n",
    "\n",
    "for a, b in zip(testes, resposta_3):\n",
    "    print(f\"A frase '{a}' é: {b}\")\n",
    "print('-----------------------------------------------------------------')\n",
    "for a, b in zip(testes, resposta_4):\n",
    "    print(f\"A frase '{a}' é: {b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A frase 'Esse governo está no início, vamos ver o que vai dar' é: Neutro\n",
      "A frase 'Estou muito feliz com o governo de Minas esse ano' é: Neutro\n",
      "A frase 'O estado de Minas Gerais decretou calamidade financeira!!!' é: Negativo\n",
      "A frase 'A segurança desse país está deixando a desejar' é: Neutro\n",
      "A frase 'O governador de Minas é mais uma vez do PT' é: Neutro\n",
      "A frase 'Homem é preso em bh' é: Positivo\n",
      "A frase ',,,,,' é: Neutro\n",
      "A frase 'Cartão BHBus dará vantagens para usuários do transporte coletivo de BH' é: Neutro\n",
      "A frase 'funk de bh = melhor do brasil' é: Neutro\n",
      "A frase 'queria ir em bh de novo :c' é: Neutro\n",
      "A frase 'Partiu BH ✈' é: Neutro\n",
      "-----------------------------------------------------------------\n",
      "A frase 'Esse governo está no início, vamos ver o que vai dar' é: Neutro\n",
      "A frase 'Estou muito feliz com o governo de Minas esse ano' é: Neutro\n",
      "A frase 'O estado de Minas Gerais decretou calamidade financeira!!!' é: Negativo\n",
      "A frase 'A segurança desse país está deixando a desejar' é: Neutro\n",
      "A frase 'O governador de Minas é mais uma vez do PT' é: Neutro\n",
      "A frase 'Homem é preso em bh' é: Positivo\n",
      "A frase ',,,,,' é: Neutro\n",
      "A frase 'Cartão BHBus dará vantagens para usuários do transporte coletivo de BH' é: Neutro\n",
      "A frase 'funk de bh = melhor do brasil' é: Neutro\n",
      "A frase 'queria ir em bh de novo :c' é: Neutro\n",
      "A frase 'Partiu BH ✈' é: Neutro\n"
     ]
    }
   ],
   "source": [
    "# Primeiro Modelo\n",
    "modelo_random = RandomForestClassifier()\n",
    "modelo_random.fit(x_transf, y_final)\n",
    "\n",
    "# Segundo Modelo\n",
    "modelo_random_2 = RandomForestClassifier()\n",
    "modelo_random_2.fit(x_transf_2, y_final)\n",
    "\n",
    "# Previsão\n",
    "resposta_5 = le.inverse_transform(modelo_random.predict(testes_vect))\n",
    "resposta_6 = le.inverse_transform(modelo_random_2.predict(testes_vect_2))\n",
    "\n",
    "for a, b in zip(testes, resposta_5):\n",
    "    print(f\"A frase '{a}' é: {b}\")\n",
    "print('-----------------------------------------------------------------')\n",
    "for a, b in zip(testes, resposta_6):\n",
    "    print(f\"A frase '{a}' é: {b}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB 1 é 45.45454545454545\n",
      "NB 2 é 63.63636363636363\n",
      "---------------------------------\n",
      "SVM 1 é 36.36363636363637\n",
      "SVM 2 é 45.45454545454545\n",
      "---------------------------------\n",
      "RandomForest 1 é 63.63636363636363\n",
      "RandomForest 2 é 63.63636363636363\n"
     ]
    }
   ],
   "source": [
    "# Acurácia \n",
    "acuracia_nb_1 = accuracy_score(modelo_nb.predict(testes_vect), resposta_teste)\n",
    "acuracia_nb_2 = accuracy_score(modelo_nb_2.predict(testes_vect_2), resposta_teste)\n",
    "acuracia_svm_1 = accuracy_score(modelo_svc.predict(testes_vect), resposta_teste)\n",
    "acuracia_svm_2 = accuracy_score(modelo_svc_2.predict(testes_vect_2), resposta_teste)\n",
    "acuracia_ran_1 = accuracy_score(modelo_random.predict(testes_vect), resposta_teste)\n",
    "acuracia_ran_2 = accuracy_score(modelo_random_2.predict(testes_vect_2), resposta_teste)\n",
    "\n",
    "# Printando os valores\n",
    "print(f'NB 1 é {acuracia_nb_1*100}')\n",
    "print(f'NB 2 é {acuracia_nb_2*100}')\n",
    "print('---------------------------------')\n",
    "print(f'SVM 1 é {acuracia_svm_1*100}')\n",
    "print(f'SVM 2 é {acuracia_svm_2*100}')\n",
    "print('---------------------------------')\n",
    "print(f'RandomForest 1 é {acuracia_ran_1*100}')\n",
    "print(f'RandomForest 2 é {acuracia_ran_2*100}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Otimazação de Hiperparâmetro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 100}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parâmetro do RandomForestClassifier\n",
    "param_grid = [{'n_estimators': [100,200]}]\n",
    "\n",
    "# Fazendo o GridSearch\n",
    "grid = GridSearchCV(modelo_random_2,param_grid).fit(x_transf_2, y_final)\n",
    "\n",
    "# Melhor parâmetro\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvando o Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando o modelo\n",
    "arquivo = 'modelos/modelo_random_2.sav'\n",
    "pickle.dump(modelo_random_2, open(arquivo,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carregando o modelo\n",
    "classificador = pickle.load(open(arquivo,'rb'))\n",
    "\n",
    "# Teste rápido\n",
    "testar = count_vect_2.transform(['BH é a capital de Minas Gerais'])\n",
    "classificador.predict(testar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referência\n",
    "- https://www.youtube.com/watch?v=Zy5sHkr5W7s\n",
    "- https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "- https://www.youtube.com/watch?v=ULmezlBTtQs\n",
    "- https://medium.com/tableless/classificando-o-conteudo-do-stack-exchange-8f3ea68fb2af\n",
    "- https://medium.com/tableless/classificando-o-conteudo-do-stack-exchange-02-6d9975e00ff7\n",
    "- https://medium.com/tableless/classificando-o-conteudo-do-stack-exchange-2ad62aba0638\n",
    "- https://minerandodados.com.br/o-que-e-data-science/#:~:text=Etapas%20e%20Processos%20de%20Data,trabalhosas%20etapas%20de%20um%20projeto.\n",
    "- https://www.youtube.com/watch?v=WKoeBgKC0o0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
